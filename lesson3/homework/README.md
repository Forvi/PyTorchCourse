Лавров Никита, РИ-230944

# Полносвязные сети

## Задание 1: Эксперименты с глубиной сети

#### 1.1 Сравнение моделей разной глубины

Задача: создать и обучить модели с различным количеством слоев

- 1 слой (линейный классификатор)
- 2 слоя (1 скрытый)
- 3 слоя (2 скрытых)
- 5 слоев (4 скрытых)
- 7 слоев (6 скрытых)

1. Однослойная модель
Для однослойной модели был создан конфиг
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": []
}
```
Здесь единственным слоем является линейный классификатор
```python
torch.nn.Linear(...)
```
Обучение проиcходило на двух датасетах
- MNIST
- CIFAR


#### MNIST
Для первого датасета метрики вышли следущими
```
Время обучения (10 эпох): 37.35 секунд
Точность на train: 0.9232
Точность на test: 0.9230
```
при параметрах
```
Количество батчей: 1024
Learning rate: 0.001
Количество эпох: 10
```

Однослойная модель хорошо себя показала - точность почти одинакова, а время обучения на 10 эпох заняло 37.35 секунд (CPU), что также неплохо. Однако, для лучшего результата необходимо улучшать модель, добавляя скрытые  слои.

![Однослойная_MNIST](/lesson3/homework/plots/1_layer_mnist.png)

Далее я решил скорректировать параметры
```
Количество батчей: 128
Learning rate: 0.005
Количество эпох: 20
```
Метрики
```
Время обучения (20 эпох): 79.60 секунд
Точность на train: 91.9332
Точность на test: 91.3843
Loss на train: 0.3000
Loss на test: 0.3432
```

Время обучения увеличилось в 2 раза, объясняется это количеством эпох.
Модель также показала себя хорошо, однако на графике кривая test нестабильна и скачет вверх-вниз. Метрики хорошие, а происходить это может потому что метрика считается реже, на фиксированном наборе данных, и небольшие изменения в весах могут заметно влиять на результат, а повышение learning rate для этого теста могут подтвердить это.

![Однослойная_MNIST3](/lesson3/homework/plots/1_layer_mnist2.png)


Последний тест на этом датасете я решил сделать с параметрами
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
Сделано это для достижения максимальной скорости и качества обучения, метрики следующие
```
Время обучения (10 эпох): 40.08 секунд
Точность на train: 0.9295
Точность на test: 0.9244
Loss на train: 0.2573
Loss на test: 0.2721
```

Время обучения опустилось до 40 секунд, что почти в 2 меньше, а метрики стали лучше (Loss ниже). 
![Однослойная_MNIST3](/lesson3/homework/plots/1_layer_mnist3.png)



В целом модель хорошо обобщает данные из MNIST-датасета и показывает хороший результат даже с простымми моделями.

#### CIFAR
Начать я решил с тех же параметров, что и для прошлого датасета
```
Количество батчей: 1024
Learning rate: 0.001
Количество эпох: 10
```

Метрики
```
Время обучения (10 эпох): 42.11 секунд
Точность на train: 0.4249
Точность на test: 0.3868
Loss на train: 1.6907
Loss на test: 1.7714
```

В случае CIFAR метрики значительно хуже, однако модель не переобучилась, а просто плохо находит зависимости в данных.

Объяснить это можно тем, что MNIST - это чёрно-белые изображения рукописных цифр 28×28, а CIFAR-10 - это цветные изображения 32×32 с 10 классами объектов (самолёты, животные, машины и т.д), где объекты имеют гораздо более сложные и разнообразные признаки. 

![Однослойная_CIFAR1](/lesson3/homework/plots/1_layer_cifar.png)


Тем не менее я попробовал перебрать параметры и добился следующего
```
Время обучения (100 эпох): 498.19 секунд
Точность на train: 0.3956
Точность на test: 0.3612
Loss на train: 1.7998
Loss на test: 1.9421
```

при параметрах 
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 100
```

Повышать количество эпох бессмысленно, так как уже на 20 эпохе метрики оставались почти неизменны, LOSS не изменился, а точность стала немного ниже. 

2. Двухслойная модель

#### MNIST
Конфиг
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 128}, 
        {"type": "relu"}            
    ]
}

```
```
FCN(
  (layers): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=10, bias=True)
  )
)
```

Rectified Linear Unit — это наиболее часто используемая функция активации при глубоком обучении. Данная функция возвращает 0, если принимает отрицательный аргумент, в случае же положительного аргумента, функция возвращает само число. 

$$
f(x) = max(0,x)
$$

Метрики 
```
Время обучения (10 эпох): 42.18 секунд
Точность на train: 0.9938
Точность на test: 0.9756
Loss на train: 0.0206
Loss на test: 0.0789
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Данная модель показа очень хорошие результаты:
- Высокая точность - на обучающей выборке (99.38%) и тестовой (97.56%)
- Низкие значения потерь
- Оптимальное время обучения

ReLU позволила добавить нелинейности в модель, тем самым улучшив показатели до почти максимального результата.

![Двухслойная_MNIST](/lesson3/homework/plots/2_layer_mnist.png)


### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 49.64 секунд
Точность на train: 0.6200
Точность на test: 0.5083
Loss на train: 1.0950
Loss на test: 1.4682
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Время опять же стандартное для 10 эпох на моём устройстве, а метрики говорят о плохой обобщаемости, а если посмотреть на график, то будет видно переобучение. Train Loss стабильно убывает, что говорит о том, что модель хорошо учится на обучающей выборке, но Test Loss почти не уменьшается, а после 4-й эпохи даже слегка растёт - это признак переобучения, модель всё лучше запоминает обучающую выборку, но не учится обобщать на новые данные.


![Двухслойная_MNIST](/lesson3/homework/plots/2_layer_cifar.png)


3. Трехслойная модель

```json
{
    "input_size": 3072,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 1024},
        {"type": "relu"},
        {"type": "linear", "size": 512},
        {"type": "relu"}
    ]
}

```

### MNIST
Параметры и метрики
```
Время обучения (10 эпох): 63.49 секунд
Точность на train: 0.9940
Точность на test: 0.9802
Loss на train: 0.0203
Loss на test: 0.0845
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

После добавления ещё одного слоя, модель начала чуть хуже обобщать данные. По метрикам этого не видно, но если взглянуть на график, то все сразу понятно. Модель начала переобучаться.

![Трехслойная_MNIST](/lesson3/homework/plots/3_layer_mnist.png)

### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 95.23 секунд
Точность на train: 0.6895
Точность на test: 0.5304
Loss на train: 0.8657
Loss на test: 1.5295
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель все так же переобучается. Время обучения увеличивается с нарастанием слоев.

![Трехслойная_CIFAR](/lesson3/homework/plots/3_layer_cifar.png)

4. Пятислойная модель

```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 512},
        {"type": "relu"},
        {"type": "linear", "size": 256},
        {"type": "relu"},
        {"type": "linear", "size": 128},
        {"type": "relu"},
        {"type": "linear", "size": 64},
        {"type": "relu"}
    ]
}
```

### MNIST
Параметры и метрики
```
Время обучения (10 эпох): 52.30 секунд
Точность на train: 0.9924
Точность на test: 0.9802
Loss на train: 0.0228
Loss на test: 0.0789
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Метрики примерно такие же как и в прошлый раз, но на графике ситуация лучше, хоть и все равно видно небольшое перебучение, тем не менее результаты модель показыват очень хорошие.

![Пятислойная_MNIST](/lesson3/homework/plots/5_layer_mnist.png)


### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 68.46 секунд
Точность на train: 0.7071
Точность на test: 0.5427
Loss на train: 0.8216
Loss на test: 1.4461
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Метрики и график не изменились

![Пятислойная_CIFAR](/lesson3/homework/plots/5_layer_cifar.png)


5. Семислойная модель

```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 512},
        {"type": "relu"},
        {"type": "linear", "size": 256},
        {"type": "relu"},
        {"type": "linear", "size": 128},
        {"type": "relu"},
        {"type": "linear", "size": 64},
        {"type": "relu"},
        {"type": "linear", "size": 32},
        {"type": "relu"},
        {"type": "linear", "size": 16},
        {"type": "relu"}
    ]
}

```

### MNIST

Параметры и метрики

```
Время обучения (10 эпох): 53.50 секунд
Точность на train: 0.9934
Точность на test: 0.9813
Loss на train: 0.0220
Loss на test: 0.0762
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель вновь не изменилась, но по графику признаки переобучения уменьшились.

![Семислойная_MNIST](/lesson3/homework/plots/7_layer_mnist.png)


### CIFAR
```
Время обучения (10 эпох): 68.63 секунд
Точность на train: 0.6908
Точность на test: 0.5397
Loss на train: 0.8714
Loss на test: 1.4103
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
 
Если сравнить с прошлыми метриками, модель стала стабильнее (об этом свидетельствует плавная кривая на графике), но она все так же склонна к переобучению. Для данного датасета нужны другие методы оптимизации, либо другие модели. 

![Семислойная_CIFAR](/lesson3/homework/plots/7_layer_cifar.png)


----

### 2.1 Анализ переобучения

MNIST
Для этого датасета оптимальным является 2 слоя, иначе повышается время обучения, затраты ресурсов на вычисления и появляется риск переобучения при таких же итоговых метриках.

CIFAR
Переобучение проявляется в значительном разрыве между точностью и потерями на обучающей и тестовой выборках, когда модель слишком хорошо запоминает тренировочные данные, но плохо обобщает на новые. Это приводит к снижению качества на тесте и нестабильности результатов при увеличении сложности модели. Для борьбы с переобучением необходимы методы регуляризации, аугментация данных и более подходящие архитектуры.

Далее я добавил dropout и batchnorm

```json
[
  {"type": "linear", "size": 512},
  {"type": "batchnorm"},
  {"type": "relu"},
  {"type": "dropout", "p": 0.5},
  {"type": "linear", "size": 256},
  {"type": "batchnorm"},
  {"type": "relu"},
  {"type": "dropout", "p": 0.5}
]
```

### CIFAR
```
Время обучения (10 эпох): 72.24 секунд
Точность на train: 0.4306
Точность на test: 0.4717
Loss на train: 1.6019
Loss на test: 1.4997
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
 
После добавления данных слоев, модель перестала переобучаться и стала более устойчивой. Кривая Test идеально повторяет Train. 

![Семислойная_CIFAR](/lesson3/homework/plots/dropout_layer_cifar.png)

### MNIST

Параметры и метрики

```
Время обучения (10 эпох): 53.50 секунд
Точность на train: 0.9934
Точность на test: 0.9813
Loss на train: 0.0220
Loss на test: 0.0762
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель тоже перестала показывать признаки переобучения.

![Семислойная_CIFAR](/lesson3/homework/plots/dropout_layer_mnist.png)

---

В итоге dropout и batchnorm оказались эффективными методами для борьбы с переобучением.

