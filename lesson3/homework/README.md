Лавров Никита, РИ-230944

# Полносвязные сети

## Задание 1: Эксперименты с глубиной сети

#### 1.1 Сравнение моделей разной глубины

Задача: создать и обучить модели с различным количеством слоев

- 1 слой (линейный классификатор)
- 2 слоя (1 скрытый)
- 3 слоя (2 скрытых)
- 5 слоев (4 скрытых)
- 7 слоев (6 скрытых)

1. Однослойная модель
Для однослойной модели был создан конфиг
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": []
}
```
Здесь единственным слоем является линейный классификатор
```python
torch.nn.Linear(...)
```
Обучение проиcходило на двух датасетах
- MNIST
- CIFAR


#### MNIST
Для первого датасета метрики вышли следущими
```
Время обучения (10 эпох): 37.35 секунд
Точность на train: 0.9232
Точность на test: 0.9230
```
при параметрах
```
Количество батчей: 1024
Learning rate: 0.001
Количество эпох: 10
```

Однослойная модель хорошо себя показала - точность почти одинакова, а время обучения на 10 эпох заняло 37.35 секунд (CPU), что также неплохо. Однако, для лучшего результата необходимо улучшать модель, добавляя скрытые  слои.

![Однослойная_MNIST](/lesson3/homework/plots/task1/1_layer_mnist.png)

Далее я решил скорректировать параметры
```
Количество батчей: 128
Learning rate: 0.005
Количество эпох: 20
```
Метрики
```
Время обучения (20 эпох): 79.60 секунд
Точность на train: 91.9332
Точность на test: 91.3843
Loss на train: 0.3000
Loss на test: 0.3432
```

Время обучения увеличилось в 2 раза, объясняется это количеством эпох.
Модель также показала себя хорошо, однако на графике кривая test нестабильна и скачет вверх-вниз. Метрики хорошие, а происходить это может потому что метрика считается реже, на фиксированном наборе данных, и небольшие изменения в весах могут заметно влиять на результат, а повышение learning rate для этого теста могут подтвердить это.

![Однослойная_MNIST3](/lesson3/homework/plots/task1/1_layer_mnist2.png)


Последний тест на этом датасете я решил сделать с параметрами
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
Сделано это для достижения максимальной скорости и качества обучения, метрики следующие
```
Время обучения (10 эпох): 40.08 секунд
Точность на train: 0.9295
Точность на test: 0.9244
Loss на train: 0.2573
Loss на test: 0.2721
```

Время обучения опустилось до 40 секунд, что почти в 2 меньше, а метрики стали лучше (Loss ниже). 
![Однослойная_MNIST3](/lesson3/homework/plots/task1/1_layer_mnist3.png)



В целом модель хорошо обобщает данные из MNIST-датасета и показывает хороший результат даже с простымми моделями.

#### CIFAR
Начать я решил с тех же параметров, что и для прошлого датасета
```
Количество батчей: 1024
Learning rate: 0.001
Количество эпох: 10
```

Метрики
```
Время обучения (10 эпох): 42.11 секунд
Точность на train: 0.4249
Точность на test: 0.3868
Loss на train: 1.6907
Loss на test: 1.7714
```

В случае CIFAR метрики значительно хуже, однако модель не переобучилась, а просто плохо находит зависимости в данных.

Объяснить это можно тем, что MNIST - это чёрно-белые изображения рукописных цифр 28×28, а CIFAR-10 - это цветные изображения 32×32 с 10 классами объектов (самолёты, животные, машины и т.д), где объекты имеют гораздо более сложные и разнообразные признаки. 

![Однослойная_CIFAR1](/lesson3/homework/plots/task1/1_layer_cifar.png)


Тем не менее я попробовал перебрать параметры и добился следующего
```
Время обучения (100 эпох): 498.19 секунд
Точность на train: 0.3956
Точность на test: 0.3612
Loss на train: 1.7998
Loss на test: 1.9421
```

при параметрах 
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 100
```

Повышать количество эпох бессмысленно, так как уже на 20 эпохе метрики оставались почти неизменны, LOSS не изменился, а точность стала немного ниже. 

2. Двухслойная модель

#### MNIST
Конфиг
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 128}, 
        {"type": "relu"}            
    ]
}

```
```
FCN(
  (layers): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=10, bias=True)
  )
)
```

Rectified Linear Unit — это наиболее часто используемая функция активации при глубоком обучении. Данная функция возвращает 0, если принимает отрицательный аргумент, в случае же положительного аргумента, функция возвращает само число. 

$$
f(x) = max(0,x)
$$

Метрики 
```
Время обучения (10 эпох): 42.18 секунд
Точность на train: 0.9938
Точность на test: 0.9756
Loss на train: 0.0206
Loss на test: 0.0789
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Данная модель показа очень хорошие результаты:
- Высокая точность - на обучающей выборке (99.38%) и тестовой (97.56%)
- Низкие значения потерь
- Оптимальное время обучения

ReLU позволила добавить нелинейности в модель, тем самым улучшив показатели до почти максимального результата.

![Двухслойная_MNIST](/lesson3/homework/plots/task1/2_layer_mnist.png)


### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 49.64 секунд
Точность на train: 0.6200
Точность на test: 0.5083
Loss на train: 1.0950
Loss на test: 1.4682
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Время опять же стандартное для 10 эпох на моём устройстве, а метрики говорят о плохой обобщаемости, а если посмотреть на график, то будет видно переобучение. Train Loss стабильно убывает, что говорит о том, что модель хорошо учится на обучающей выборке, но Test Loss почти не уменьшается, а после 4-й эпохи даже слегка растёт - это признак переобучения, модель всё лучше запоминает обучающую выборку, но не учится обобщать на новые данные.


![Двухслойная_MNIST](/lesson3/homework/plots/task1/2_layer_cifar.png)


3. Трехслойная модель

```json
{
    "input_size": 3072,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 1024},
        {"type": "relu"},
        {"type": "linear", "size": 512},
        {"type": "relu"}
    ]
}

```

### MNIST
Параметры и метрики
```
Время обучения (10 эпох): 63.49 секунд
Точность на train: 0.9940
Точность на test: 0.9802
Loss на train: 0.0203
Loss на test: 0.0845
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

После добавления ещё одного слоя, модель начала чуть хуже обобщать данные. По метрикам этого не видно, но если взглянуть на график, то все сразу понятно. Модель начала переобучаться.

![Трехслойная_MNIST](/lesson3/homework/plots/task1/3_layer_mnist.png)

### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 95.23 секунд
Точность на train: 0.6895
Точность на test: 0.5304
Loss на train: 0.8657
Loss на test: 1.5295
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель все так же переобучается. Время обучения увеличивается с нарастанием слоев.

![Трехслойная_CIFAR](/lesson3/homework/plots/task1/3_layer_cifar.png)

4. Пятислойная модель

```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 512},
        {"type": "relu"},
        {"type": "linear", "size": 256},
        {"type": "relu"},
        {"type": "linear", "size": 128},
        {"type": "relu"},
        {"type": "linear", "size": 64},
        {"type": "relu"}
    ]
}
```

### MNIST
Параметры и метрики
```
Время обучения (10 эпох): 52.30 секунд
Точность на train: 0.9924
Точность на test: 0.9802
Loss на train: 0.0228
Loss на test: 0.0789
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Метрики примерно такие же как и в прошлый раз, но на графике ситуация лучше, хоть и все равно видно небольшое перебучение, тем не менее результаты модель показыват очень хорошие.

![Пятислойная_MNIST](/lesson3/homework/plots/task1/5_layer_mnist.png)


### CIFAR
Параметры и метрики
```
Время обучения (10 эпох): 68.46 секунд
Точность на train: 0.7071
Точность на test: 0.5427
Loss на train: 0.8216
Loss на test: 1.4461
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Метрики и график не изменились

![Пятислойная_CIFAR](/lesson3/homework/plots/task1/5_layer_cifar.png)


5. Семислойная модель

```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 512},
        {"type": "relu"},
        {"type": "linear", "size": 256},
        {"type": "relu"},
        {"type": "linear", "size": 128},
        {"type": "relu"},
        {"type": "linear", "size": 64},
        {"type": "relu"},
        {"type": "linear", "size": 32},
        {"type": "relu"},
        {"type": "linear", "size": 16},
        {"type": "relu"}
    ]
}

```

### MNIST

Параметры и метрики

```
Время обучения (10 эпох): 53.50 секунд
Точность на train: 0.9934
Точность на test: 0.9813
Loss на train: 0.0220
Loss на test: 0.0762
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель вновь не изменилась, но по графику признаки переобучения уменьшились.

![Семислойная_MNIST](/lesson3/homework/plots/task1/7_layer_mnist.png)


### CIFAR
```
Время обучения (10 эпох): 68.63 секунд
Точность на train: 0.6908
Точность на test: 0.5397
Loss на train: 0.8714
Loss на test: 1.4103
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
 
Если сравнить с прошлыми метриками, модель стала стабильнее (об этом свидетельствует плавная кривая на графике), но она все так же склонна к переобучению. Для данного датасета нужны другие методы оптимизации, либо другие модели. 

![Семислойная_CIFAR](/lesson3/homework/plots/task1/7_layer_cifar.png)


----

### 2.1 Анализ переобучения

MNIST
Для этого датасета оптимальным является 2 слоя, иначе повышается время обучения, затраты ресурсов на вычисления и появляется риск переобучения при таких же итоговых метриках. После добавления 2 и более скрытых слоев модель начинает показывать признаки переобучения, а кривая обучения на графике становится менее плавной.

CIFAR
На данном датасете модель показывать явное переобучение, особенно это видно на графиках - после 2 эпохи кривая либо колеблится у одного значения, либо уходит в совсем другую сторону. На CIFAR модель показывала сильные признаки переобучения с самого начала, то есть с 1 слоя. Здесь оптимальной глубиной является 7 слоев, так как в таком случае модель начинает лучше обобщать данные, однако результат все равно не хорош и лучше воспользоваться другими моделями либо менять архитектуру.

Далее я добавил dropout и batchnorm

```json
[
  {"type": "linear", "size": 512},
  {"type": "batchnorm"},
  {"type": "relu"},
  {"type": "dropout", "p": 0.5},
  {"type": "linear", "size": 256},
  {"type": "batchnorm"},
  {"type": "relu"},
  {"type": "dropout", "p": 0.5}
]
```

### CIFAR
```
Время обучения (10 эпох): 72.24 секунд
Точность на train: 0.4306
Точность на test: 0.4717
Loss на train: 1.6019
Loss на test: 1.4997
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```
 
После добавления данных слоев, модель перестала переобучаться и стала более устойчивой. Кривая Test идеально повторяет Train. 

![Семислойная_CIFAR](/lesson3/homework/plots/task1/dropout_layer_cifar.png)

### MNIST

Параметры и метрики

```
Время обучения (10 эпох): 53.50 секунд
Точность на train: 0.9934
Точность на test: 0.9813
Loss на train: 0.0220
Loss на test: 0.0762
```

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

Модель тоже перестала показывать признаки переобучения.

![Семислойная_CIFAR](/lesson3/homework/plots/task1/dropout_layer_mnist.png)

---

В итоге dropout и batchnorm оказались эффективными методами для борьбы с переобучением.

----
### Задание 2: Эксперименты с шириной сети 

В данном задании необходимо рассмотреть различия от различной ширины модели. Нужно использовать 3 слоя. 

- Узкие слои: [64, 32, 16]
- Средние слои: [256, 128, 64]
- Широкие слои: [1024, 512, 256]
- Очень широкие слои: [2048, 1024, 512]

Проверка будет проходить со следующими параметрами
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

#### 1. Узкие слои
Конфигурация
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 64},
        {"type": "relu"},
        {"type": "linear", "size": 32},
        {"type": "relu"},
        {"type": "linear", "size": 16},
        {"type": "relu"}
    ]
}
```
#### MNIST

Итог
```
Количество параметров модели: 53018
Время обучения (10 эпох): 48.61 секунд
Точность на train: 0.9840
Точность на test: 0.9733
Loss на train: 0.0503
Loss на test: 0.0924
```

Модель с небольшой шириной слоёв демонстрирует эффективное обучение и хорошее обобщение на датасете MNIST, достигая хорошей точности - около 97-98%. При этом модель остаётся компактной, а именно 53018 параметров, что положительно сказывается на скорости обучения (48.61 сек.) и вычислительных ресурсах. 

![low_MNIST](/lesson3/homework/plots/task2/low_mnist.png)

#### CIFAR
```
Количество параметров модели: 199450
Время обучения (10 эпох): 49.40 секунд
Точность на train: 0.5786
Точность на test: 0.4996
Loss на train: 1.1886
Loss на test: 1.4415
```

Модель до сих пор недообучается и страдает от переобучения. Количество параметров значительно выше по причине большего колчества входных признаков, но в целом время обучения небольшое, а график как и впрошлых эксперементах. 

![low_CIFAR](/lesson3/homework/plots/task2/low_cifar.png)


#### 2. Средние слои
Конфигурация
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 256},
        {"type": "relu"},
        {"type": "linear", "size": 128},
        {"type": "relu"},
        {"type": "linear", "size": 64},
        {"type": "relu"}
    ]
}   
```
#### MNIST
Итог
```
Количество параметров модели: 242762
Время обучения (10 эпох): 50.32 секунд
Точность на train: 0.9922
Точность на test: 0.9790
Loss на train: 0.0232
Loss на test: 0.0809
```

Конфигурация со средней шириной дала значительное увеличение параметров (242762), что в 4.5 больше прошлой. Также модель стала лучше обучаться, но повысился шанс переобучения, об этом говорит график. Не смотря на большое количество параметров, модель обучается довольно быстро и вреся повысилось всего на 2 секунды.

![middle_MNIST](/lesson3/homework/plots/task2/middle_mnist.png)

#### CIFAR
```
Количество параметров модели: 828490
Время обучения (10 эпох): 59.86 секунд
Точность на train: 0.6832
Точность на test: 0.5393
Loss на train: 0.8966
Loss на test: 1.4526
```

Модель со средней шириной слоёв показывает улучшение по сравнению с узкой, но всё ещё не достигает высокого качества на этом датасете. Количество параметров заметно больше и время обучения увеличилось, что, как поняли раннее, естественно при росте параметров. 

![middle_CIFAR](/lesson3/homework/plots/task2/middle_cifar.png)


#### 3. Широкие слои

Конфигурация
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 1024},
        {"type": "relu"},
        {"type": "linear", "size": 512},
        {"type": "relu"},
        {"type": "linear", "size": 256},
        {"type": "relu"}
    ]
}
```


#### MNIST
Итог
```
Количество параметров модели: 1462538
Время обучения (10 эпох): 71.31 секунд
Точность на train: 0.9933
Точность на test: 0.9808
Loss на train: 0.0207
Loss на test: 0.0775

```

Увеличение ширины слоёв немного повысило качество модели на MNIST. Количестов параметров также сильно выросло, но время обучения остается невысоким.

![big_MNIST](/lesson3/homework/plots/task2/big_mnist.png)

#### CIFAR
```
Количество параметров модели: 3805450
Время обучения (10 эпох): 95.80 секунд
Точность на train: 0.7081
Точность на test: 0.5380
Loss на train: 0.8090
Loss на test: 1.5490
```

Как и в прошлые разы модель с широкими слоями на CIFAR демонстрирует рост точности на train, но слабое обобщение на test, несмотря на значительный рост параметров и времени обучения.

![big_CIFAR](/lesson3/homework/plots/task2/big_cifar.png)


#### 3. Очень широкие слои

Конфигурация
```json
{
    "input_size": 784,
    "num_classes": 10,
    "layers": [
        {"type": "linear", "size": 2048},
        {"type": "relu"},
        {"type": "linear", "size": 1024},
        {"type": "relu"},
        {"type": "linear", "size": 512},
        {"type": "relu"}
    ]
}
```


#### MNIST
Итог
```
Количество параметров модели: 4235786
Время обучения (10 эпох): 127.14 секунд
Точность на train: 0.9915
Точность на test: 0.9810
Loss на train: 0.0285
Loss на test: 0.0914

```

![verybig_MNIST](/lesson3/homework/plots/task2/verybig_mnist.png)

#### CIFAR
```
Количество параметров модели: 8921610
Время обучения (10 эпох): 187.19 секунд
Точность на train: 0.7010
Точность на test: 0.5274
Loss на train: 0.8371
Loss на test: 1.5819
```

![verybig_CIFAR](/lesson3/homework/plots/task2/verybig_cifar.png)

---
### Вывод
Увеличение ширины слоёв повышает качество моделей, что соответствует наблюдениям из исследований - с ростом числа нейронов точность тестовой выборки обычно повышается, но при этом возрастает риск переобучения и увеличивается время обучения. В случае MNIST модель демонстрирует баланс между высокой точностью и временем обучения, при этом переобучение не выражено явно. В случае CIFAR модель изначально имела высокий уровень переобучения, а с увеличением ширины слоев этот фактор становится ещё выше, однако увроень недообучения у модели становится ниже. Также стоит отметить, что при увеличении ширины слоёв, количество параметров, а соответственно и время обучения также увеличивается.


----
### Задание 3: Эксперименты с регуляризацией

Исследуйте различные техники регуляризации:
- Без регуляризации
- Только Dropout (разные коэффициенты: 0.1, 0.3, 0.5)
- Только BatchNorm
- Dropout + BatchNorm
- L2 регуляризация (weight decay)

```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
```

#### Без регуляризации

#### MNIST

Итог
```
Количество параметров модели: 242762
Время обучения (10 эпох): 53.16 секунд
Точность на train: 0.9932
Точность на test: 0.9775
Loss на train: 0.0208
Loss на test: 0.0893
```

Метрики стандартные для модели с такой конфигурацией. Подвержена переобучению.

![def_MNIST](/lesson3/homework/plots/task3/dropout_def_mnist.png)

#### CIFAR

Итог
```
Количество параметров модели: 828490
Время обучения (10 эпох): 58.72 секунд
Точность на train: 0.6774
Точность на test: 0.5275
Loss на train: 0.9038
Loss на test: 1.4302
```

Метрики стандартные для модели с такой конфигурацией, рассмотренно раннее (без dropout и BatchNorm) 

![def_CIFAR](/lesson3/homework/plots/task3/dropout_def_cifar.png)

#### Dropout

#### 0.1

Итог
```
Время обучения (10 эпох): 53.77 секунд
Точность на train: 0.9888
Точность на test: 0.9782
Loss на train: 0.0357
Loss на test: 0.0811
```

 В 1 задании рассматривался варинт с добавлением dropout, его добавление значительно уменьшает вероятность переобучения, этот раз не исключение.

![def_MNIST](/lesson3/homework/plots/task3/dropout_01_mnist.png)

#### CIFAR

Итог
```
Количество параметров модели: 828490
Время обучения (10 эпох): 60.31 секунд
Точность на train: 0.5841
Точность на test: 0.5369
Loss на train: 1.1655
Loss на test: 1.3363
```

Добавление dropout значительно уменьшила уровень переобучения у модели, особенно хорошо это видно на графике.

![def_CIFAR](/lesson3/homework/plots/task3/dropout_01_cifar.png)



#### 0.3

Итог
```
Время обучения (10 эпох): 53.77 секунд
Точность на train: 0.9888
Точность на test: 0.9782
Loss на train: 0.0357
Loss на test: 0.0811
```

Ниже уровень переобучения в сравнении с p=0.1, высокий уровень обученности.

![def_MNIST](/lesson3/homework/plots/task3/dropout_03_mnist.png)

#### CIFAR

Итог
```
Количество параметров модели: 828490
Время обучения (10 эпох): 59.87 секунд
Точность на train: 0.4961
Точность на test: 0.5056
Loss на train: 1.4275
Loss на test: 1.3910
```

Теперь модель не переобучается, осталась лишь проблема недообученности

![def_CIFAR](/lesson3/homework/plots/task3/dropout_03_cifar.png)



#### 0.5

Итог
```
Количество параметров модели: 242762
Время обучения (10 эпох): 53.92 секунд
Точность на train: 0.9593
Точность на test: 0.9749
Loss на train: 0.1522
Loss на test: 0.0941
```

Не имеет переобучения, хорошо обобщает данные.

![def_MNIST](/lesson3/homework/plots/task3/dropout_05_mnist.png)

#### CIFAR

Итог
```
Количество параметров модели: 828490
Время обучения (10 эпох): 61.23 секунд
Точность на train: 0.4035
Точность на test: 0.4607
Loss на train: 1.6726
Loss на test: 1.5364
```

Модель не переобучается, прибавилась стабильность модели. Недообученность осталась.

![def_CIFAR](/lesson3/homework/plots/task3/dropout_05_cifar.png)


### Dropout + BatchNorm

#### MNIST

Итог
```
Количество параметров модели: 242762
Время обучения (10 эпох): 53.01 секунд
Точность на train: 0.9586
Точность на test: 0.9734
Loss на train: 0.1528
Loss на test: 0.0989
```

В данном случае добавление BatchNorm ничего не изменило, вероятно потому что это предел модели.

![def_MNIST](/lesson3/homework/plots/task3/dropout_batch_mnist.png)

#### CIFAR

Итог
```
Количество параметров модели: 828490
Время обучения (10 эпох): 63.25 секунд
Точность на train: 0.4086
Точность на test: 0.4693
Loss на train: 1.6744
Loss на test: 1.5466
```

В случае CIFAR BatchNorm также не дал каких-либо изменений. Смею предположить, что причина та же.

![def_CIFAR](/lesson3/homework/plots/task3/dropout_batch_cifar.png)

----

## Вывод

В данной работе я провел серию экспериментов с различными конфигурациями моделей и регуляризацией. Выяснилось, что увеличение числа слоев и ширины слоев в целом улучшает точность, но при этом значительно растет количество параметров и время обучения, что повышает риск переобучения. Для простых задач, таких как MNIST, увеличение ширины слоев дает заметный прирост качества, тогда как на более сложных данных, например CIFAR, рост параметров не всегда приводит к улучшению из-за ограничений архитектуры. Регуляризация, особенно Dropout и BatchNorm, помогает стабилизировать обучение и снизить переобучение, однако оптимальные параметры зависят от конкретной задачи и архитектуры. В целом, выбор архитектуры и регуляризации требует баланса между сложностью модели, качеством обобщения и вычислительными ресурсами.