Лавров Никита, РИ-230944

# Сверточные сети

### Задание 1: Сравнение CNN и полносвязных сетей

При обучения каждой модели использовались настройки:
~~~
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
Оптимизатор: Adam
~~~

#### 1.1 Сравнение на MNIST
Обучив обе модели на датасете MNIST, были получены следующие результаты

| Модель | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|--------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| CNN    | 0.9958            | 0.9928           | 0.0119        | 0.0260       | 421642               | 427.3445            |
| FC     | 0.9771            | 0.9820           | 0.0727        | 0.0591       | 537354               | 182.0798            |


----
- Точность

    Модель CNN превосходит FC по точности как на обучающей (на ~0.02%), так и на тестовой выборке (на ~0.01%).

- Потери

    CNN даёт меньшие потери на train и test, нежели FC, а именно на ~0.05 в тренировочной выборке и на ~0.03 в тестовой выборке, что говорит о более уверенном и стабильном обучении.

- Параметры

    У FC больше параметров (537 354 против 421 642 у CNN), но при этом качество ниже, в данном случае количество параметров не решает, так как CNN имеет более сложную архитектуру.

- Время обучения

    CNN обучается дольше (427 сек. против 182 сек. у FC), что объясняется более сложной архитектурой и свёртками, которые выполняют довольно сложный процесс вычислений. 
---
![CNN_VS_FC_MNIST](/lesson4/homework/plots/fc_vs_cnn_mnist.png)

----------------------------

#### 1.2 Сравнение на CIFAR-10
Обучив обе модели на датасете CIFAR, были получены следующие результаты

| Модель | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|--------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| CNN    | 0.8958            | 0.7678           | 0.2893        | 0.7847       | 620362               | 580.9743            |
| FC     | 0.5238            | 0.5522           | 1.3304        | 1.2711       | 1708810              | 227.9217            |



----
- Точность

    CNN значительно превосходит FC по точности на обеих выборках - на ~38% train и ~20% test. Разница особенно заметна на обучении - CNN хорошо учится на сложных изображениях, а FC практически не может извлечь полезные признаки.

- Потери

    CNN даёт гораздо меньший loss, что говорит о более уверенном и стабильном обучении. Разница на train - 1.04, test - 0.48

- Параметры

    У FC почти в 3 раза больше параметров, но качество при этом намного хуже.

- Время обучения

    CNN обучается дольше (581 сек. против 228 сек. у FC) из-за более сложных операций, но качество значительно выше.
---
![CNN_VS_FC_CIFAR](/lesson4/homework/plots/fc_vs_cnn_cifar.png)


#### Вывод
Сверточные нейронные сети более эффективные и точные для задач классификации изображений, чем полносвязные, даже при условии, что у FC больше параметров. Для CIFAR-10 разница в качестве особенно заметна - CNN достигает почти 77% точности на тесте, а FC - лишь около 55%, что лишь немного выше случайного угадывания. Однако время обучения CNN сильно выше, потому для таких простых задач как MNIST можно использовать полносвязные НС для экономии времени и ресурсов.


### Задание 2: Анализ архитектур CNN

#### 2.1 Влияние размера ядра свертки

Чтобы оценить влияние размера ядра свертки, были использованы настройки
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
Оптимизатор: Adam
stride: 1
Pooling: Max (2, 2)
```

Также стоит уточнить, что слои были настроены не под датасет CIFAR-10 для более наглядной разницы между архитектурами.

Оценивались следующие варианты

- 3x3 ядра
- 5x5 ядра
- 7x7 ядра
- Комбинация разных размеров (1x1 + 3x3)

#### Результат

| Размер свертки     | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|------------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| 3x3        | 0.8504            | 0.7373           | 0.4192        | 0.8545       | 545098               | 414.439             |
| 5x5        | 0.8381            | 0.7202           | 0.4505        | 0.9269       | 579402               | 598.991             |
| 7x7        | 0.8141            | 0.7144           | 0.5158        | 0.9180       | 630858               | 815.276             |
| 1x1 + 3x3  | 0.8703            | 0.6673           | 0.3574        | 1.2296       | 2117194              | 974.464             |


----
![KERNEL_3_CIFAR](/lesson4/homework/plots/cifar_kernel_3.png)

![KERNEL_5_CIFAR](/lesson4/homework/plots/cifar_kernel_5.png)

![KERNEL_7_CIFAR](/lesson4/homework/plots/cifar_kernel_7.png)

![KERNEL_1_3_CIFAR](/lesson4/homework/plots/cifar_kernel_comb.png)

----

#### Оценка
- Точность

    Модель с ядрами 3x3 показывает хороший баланс между точностью и временем обучения, она достигает хорошие результаты на тесте (~74%)

    Увеличение размера ядра до 5x5 и 7x7 приводит к снижению точности как на train, так и на test, при этом время обучения значительно возрастает.

    Комбинация 1x1 + 3x3 даёт наилучшую точность на train (0.8703) и наименьший loss на train, но сильно хуже результаты на тесте (~67%) и самый высокий loss на тесте, что может указывать на переобучение из-за большого числа параметров.

- Количество параметров и время обучения

    Увеличение размера ядра приводит к росту количества параметров и времени обучения.

    Самая тяжелая по параметрам и времени модель - комбинация 1x1 + 3x3, что объясняет её склонность к переобучению.

    Модель с ядрами 3x3 - самая быстрая и компактная.


#### Вывод
Для задач, где важен баланс между качеством и ресурсами, оптимальным выбором является размер ядра 3x3, т.к. оно даёт хорошую точность, приемлемое время обучения и умеренное число параметров.

Использование больших ядер (5x5, 7x7) можно использовать только при наличии достаточных вычислительных ресурсов и необходимости захвата более глобальных признаков.

Комбинация 1x1 + 3x3 требует дополнительной регуляризации и контроля переобучения, но может быть полезна для более сложных архитектур и задач.

В итоге эксперимент подтвердил, что ядра 3x3 являются оптимальным компромиссом в большинстве случаев, а увеличение ядра требует осторожного подхода и дополнительной настройки модели.


-----------
### 2.2 Влияние глубины CNN

- Неглубокая CNN (2 conv слоя)
- Средняя CNN (4 conv слоя)
- Глубокая CNN (6+ conv слоев)
- CNN с Residual связями

#### Результат

| Кол-во слоев   | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10) |
|----------------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| 2 conv         | 0.6439            | 0.6513           | 1.0014        | 0.9799       | 67,850               | 101.74              |
| 4 conv         | 0.8626            | 0.7723           | 0.3833        | 0.7233       | 692,810              | 201.66              |
| 6 conv         | 0.9168            | 0.7381           | 0.2436        | 1.0939       | 4,978,378            | 316.93              |
| Residual       | 0.8929            | 0.8068           | 0.3051        | 0.5986       | 161,482              | 646.01              |


----
![KERNEL_3_CIFAR](/lesson4/homework/plots/cifar_layers_2.png)

![KERNEL_5_CIFAR](/lesson4/homework/plots/cifar_layers_4.png)

![KERNEL_7_CIFAR](/lesson4/homework/plots/cifar_layers_6.png)

![KERNEL_1_3_CIFAR](/lesson4/homework/plots/cifar_layers_residual.png)

----

#### Оценка

- Точность

    Неглубокая CNN с 2 свёрточными слоями показывает низкую точность как на обучающей, так и на тестовой выборке (~64%), что говорит о недостаточной сложности модели для CIFAR-10 и её неспособности извлекать сложные признаки.

    При увеличении глубины до 4 слоёв наблюдается значительный рост точности: на обучении она достигает 86%, на тесте — 77%. Это свидетельствует о том, что увеличение числа слоёв позволяет сети лучше обучаться и извлекать более сложные паттерны из данных.

    Глубокая CNN с 6 слоями демонстрирует ещё более высокую точность на обучающей выборке (92%), однако точность на тесте падает до 74%, а loss на тесте заметно увеличивается. Это яркий признак переобучения: модель хорошо запоминает обучающие данные, но хуже обобщает на новых примерах.

    CNN с residual-связями показывает отличные результаты: высокая точность на обучении (89%) и самая высокая точность на тесте (81%), а также самый низкий loss на тесте (0.60). Это говорит о том, что residual-связи действительно улучшают обобщающую способность сети и помогают бороться с деградацией качества при увеличении глубины.

- Количество параметров и время обучения

    С увеличением числа слоёв резко растёт количество параметров и время обучения. Для 2 слоёв — всего 67 тыс. параметров и минимальное время обучения (102 с). Для 4 слоёв — почти 700 тыс. параметров, время обучения удваивается.

    6-слойная сеть становится очень тяжёлой: почти 5 млн параметров и время обучения увеличивается до 317 с. При этом качество на тесте не только не растёт, но и падает.

    Интересно, что сеть с residual-связями имеет относительно небольшое количество параметров (161 тыс.), но при этом обучается дольше всех (646 с), вероятно, из-за усложнённой архитектуры и большего числа операций. Однако она показывает лучший баланс между качеством и сложностью.

- Особенности

    Глубокие сети без специальных техник склонны к переобучению и деградации качества на тесте.

    Residual-связи позволяют строить более глубокие архитектуры без потери обобщающей способности и даже при меньшем числе параметров.

    Время обучения зависит не только от числа параметров, но и от сложности архитектуры (например, residual-блоки вычислительно затратнее).

#### Вывод

Для задач, где важен баланс между точностью и вычислительными ресурсами, оптимальным выбором является средняя по глубине сеть (4 свёрточных слоя) - она даёт высокую точность на тесте, разумное время обучения и не слишком велика по числу параметров.

Увеличение глубины без residual-связей приводит к переобучению и не даёт прироста качества на тесте, несмотря на рост точности на обучении.

Использование residual-связей позволяет строить более глубокие и эффективные сети, которые лучше обобщают и достигают наилучших результатов на тестовой выборке при умеренном количестве параметров.

Эксперимент подтверждает, что для CIFAR-10 важно не только увеличивать глубину сети, но и применять современные архитектурные решения (residual-связи), чтобы добиться максимального качества без избыточной сложности и переобучения.