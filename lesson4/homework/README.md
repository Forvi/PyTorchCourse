Лавров Никита, РИ-230944

# Сверточные сети

### Задание 1: Сравнение CNN и полносвязных сетей

При обучения каждой модели использовались настройки:
~~~
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
Оптимизатор: Adam
~~~

#### 1.1 Сравнение на MNIST
Обучив обе модели на датасете MNIST, были получены следующие результаты

| Модель | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|--------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| CNN    | 0.9958            | 0.9928           | 0.0119        | 0.0260       | 421642               | 427.3445            |
| FC     | 0.9771            | 0.9820           | 0.0727        | 0.0591       | 537354               | 182.0798            |


----
- Точность

    Модель CNN превосходит FC по точности как на обучающей (на ~0.02%), так и на тестовой выборке (на ~0.01%).

- Потери

    CNN даёт меньшие потери на train и test, нежели FC, а именно на ~0.05 в тренировочной выборке и на ~0.03 в тестовой выборке, что говорит о более уверенном и стабильном обучении.

- Параметры

    У FC больше параметров (537 354 против 421 642 у CNN), но при этом качество ниже, в данном случае количество параметров не решает, так как CNN имеет более сложную архитектуру.

- Время обучения

    CNN обучается дольше (427 сек. против 182 сек. у FC), что объясняется более сложной архитектурой и свёртками, которые выполняют довольно сложный процесс вычислений. 
---
![CNN_VS_FC_MNIST](/lesson4/homework/plots/fc_vs_cnn_mnist.png)

----------------------------

#### 1.2 Сравнение на CIFAR-10
Обучив обе модели на датасете CIFAR, были получены следующие результаты

| Модель | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|--------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| CNN    | 0.8958            | 0.7678           | 0.2893        | 0.7847       | 620362               | 580.9743            |
| FC     | 0.5238            | 0.5522           | 1.3304        | 1.2711       | 1708810              | 227.9217            |



----
- Точность

    CNN значительно превосходит FC по точности на обеих выборках - на ~38% train и ~20% test. Разница особенно заметна на обучении - CNN хорошо учится на сложных изображениях, а FC практически не может извлечь полезные признаки.

- Потери

    CNN даёт гораздо меньший loss, что говорит о более уверенном и стабильном обучении. Разница на train - 1.04, test - 0.48

- Параметры

    У FC почти в 3 раза больше параметров, но качество при этом намного хуже.

- Время обучения

    CNN обучается дольше (581 сек. против 228 сек. у FC) из-за более сложных операций, но качество значительно выше.
---
![CNN_VS_FC_CIFAR](/lesson4/homework/plots/fc_vs_cnn_cifar.png)


#### Вывод
Сверточные нейронные сети более эффективные и точные для задач классификации изображений, чем полносвязные, даже при условии, что у FC больше параметров. Для CIFAR-10 разница в качестве особенно заметна - CNN достигает почти 77% точности на тесте, а FC - лишь около 55%, что лишь немного выше случайного угадывания. Однако время обучения CNN сильно выше, потому для таких простых задач как MNIST можно использовать полносвязные НС для экономии времени и ресурсов.


### Задание 2: Анализ архитектур CNN

#### 2.1 Влияние размера ядра свертки

Чтобы оценить влияние размера ядра свертки, были использованы настройки
```
Количество батчей: 128
Learning rate: 0.001
Количество эпох: 10
Оптимизатор: Adam
stride: 1
Pooling: Max (2, 2)
```

Также стоит уточнить, что слои были настроены не под датасет CIFAR-10 для более наглядной разницы между архитектурами.

Оценивались следующие варианты

- 3x3 ядра
- 5x5 ядра
- 7x7 ядра
- Комбинация разных размеров (1x1 + 3x3)

#### Результат

| Размер свертки     | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|------------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| 3x3        | 0.8504            | 0.7373           | 0.4192        | 0.8545       | 545098               | 414.439             |
| 5x5        | 0.8381            | 0.7202           | 0.4505        | 0.9269       | 579402               | 598.991             |
| 7x7        | 0.8141            | 0.7144           | 0.5158        | 0.9180       | 630858               | 815.276             |
| 1x1 + 3x3  | 0.8703            | 0.6673           | 0.3574        | 1.2296       | 2117194              | 974.464             |


----
![KERNEL_3_CIFAR](/lesson4/homework/plots/cifar_kernel_3.png)

![KERNEL_5_CIFAR](/lesson4/homework/plots/cifar_kernel_5.png)

![KERNEL_7_CIFAR](/lesson4/homework/plots/cifar_kernel_7.png)

![KERNEL_1_3_CIFAR](/lesson4/homework/plots/cifar_kernel_comb.png)

----

#### Оценка
- Точность

    Модель с ядрами 3x3 показывает хороший баланс между точностью и временем обучения, она достигает хорошие результаты на тесте (~74%)

    Увеличение размера ядра до 5x5 и 7x7 приводит к снижению точности как на train, так и на test, при этом время обучения значительно возрастает.

    Комбинация 1x1 + 3x3 даёт наилучшую точность на train (0.8703) и наименьший loss на train, но сильно хуже результаты на тесте (~67%) и самый высокий loss на тесте, что может указывать на переобучение из-за большого числа параметров.

- Количество параметров и время обучения

    Увеличение размера ядра приводит к росту количества параметров и времени обучения.

    Самая тяжелая по параметрам и времени модель - комбинация 1x1 + 3x3, что объясняет её склонность к переобучению.

    Модель с ядрами 3x3 - самая быстрая и компактная.


#### Вывод
Для задач, где важен баланс между качеством и ресурсами, оптимальным выбором является размер ядра 3x3, т.к. оно даёт хорошую точность, приемлемое время обучения и умеренное число параметров.

Использование больших ядер (5x5, 7x7) можно использовать только при наличии достаточных вычислительных ресурсов и необходимости захвата более глобальных признаков.

Комбинация 1x1 + 3x3 требует дополнительной регуляризации и контроля переобучения, но может быть полезна для более сложных архитектур и задач.

В итоге эксперимент подтвердил, что ядра 3x3 являются оптимальным компромиссом в большинстве случаев, а увеличение ядра требует осторожного подхода и дополнительной настройки модели.


-----------
### 2.2 Влияние глубины CNN

- Неглубокая CNN (2 conv слоя)
- Средняя CNN (4 conv слоя)
- Глубокая CNN (6+ conv слоев)
- CNN с Residual связями

#### Результат

| Кол-во слоев   | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10) |
|----------------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| 2 conv         | 0.6439            | 0.6513           | 1.0014        | 0.9799       | 67,850               | 101.74              |
| 4 conv         | 0.8626            | 0.7723           | 0.3833        | 0.7233       | 692,810              | 201.66              |
| 6 conv         | 0.9168            | 0.7381           | 0.2436        | 1.0939       | 4,978,378            | 316.93              |
| Residual       | 0.8929            | 0.8068           | 0.3051        | 0.5986       | 161,482              | 646.01              |


----
![KERNEL_3_CIFAR](/lesson4/homework/plots/cifar_layers_2.png)

![KERNEL_5_CIFAR](/lesson4/homework/plots/cifar_layers_4.png)

![KERNEL_7_CIFAR](/lesson4/homework/plots/cifar_layers_6.png)

![KERNEL_1_3_CIFAR](/lesson4/homework/plots/cifar_layers_residual.png)

----

#### Оценка

- Точность

    Неглубокая CNN с 2 свёрточными слоями показывает низкую точность как на обучающей, так и на тестовой выборке (~64%), что говорит о недостаточной сложности модели для CIFAR-10 и её неспособности извлекать сложные признаки.

    При увеличении глубины до 4 слоёв наблюдается значительный рост точности: на обучении она достигает 86%, на тесте — 77%. Это свидетельствует о том, что увеличение числа слоёв позволяет сети лучше обучаться и извлекать более сложные паттерны из данных.

    Глубокая CNN с 6 слоями демонстрирует ещё более высокую точность на обучающей выборке (92%), однако точность на тесте падает до 74%, а loss на тесте заметно увеличивается. Это яркий признак переобучения: модель хорошо запоминает обучающие данные, но хуже обобщает на новых примерах.

    CNN с residual-связями показывает отличные результаты: высокая точность на обучении (89%) и самая высокая точность на тесте (81%), а также самый низкий loss на тесте (0.60). Это говорит о том, что residual-связи действительно улучшают обобщающую способность сети и помогают бороться с деградацией качества при увеличении глубины.

- Количество параметров и время обучения

    С увеличением числа слоёв резко растёт количество параметров и время обучения. Для 2 слоёв — всего 67 тыс. параметров и минимальное время обучения (102 с). Для 4 слоёв — почти 700 тыс. параметров, время обучения удваивается.

    6-слойная сеть становится очень тяжёлой: почти 5 млн параметров и время обучения увеличивается до 317 с. При этом качество на тесте не только не растёт, но и падает.

    Интересно, что сеть с residual-связями имеет относительно небольшое количество параметров (161 тыс.), но при этом обучается дольше всех (646 с), вероятно, из-за усложнённой архитектуры и большего числа операций. Однако она показывает лучший баланс между качеством и сложностью.

- Особенности

    Глубокие сети без специальных техник склонны к переобучению и деградации качества на тесте.

    Residual-связи позволяют строить более глубокие архитектуры без потери обобщающей способности и даже при меньшем числе параметров.

    Время обучения зависит не только от числа параметров, но и от сложности архитектуры (например, residual-блоки вычислительно затратнее).

#### Вывод

Для задач, где важен баланс между точностью и вычислительными ресурсами, оптимальным выбором является средняя по глубине сеть (4 свёрточных слоя) - она даёт высокую точность на тесте, разумное время обучения и не слишком велика по числу параметров.

Увеличение глубины без residual-связей приводит к переобучению и не даёт прироста качества на тесте, несмотря на рост точности на обучении.

Использование residual-связей позволяет строить более глубокие и эффективные сети, которые лучше обобщают и достигают наилучших результатов на тестовой выборке при умеренном количестве параметров.

Эксперимент подтверждает, что для CIFAR-10 важно не только увеличивать глубину сети, но и применять современные архитектурные решения (residual-связи), чтобы добиться максимального качества без избыточной сложности и переобучения.

-------

### Задание 3: Кастомные слои и эксперименты

Данный эксперимент посвящён сравнению различных моделей CNN, используя стандартную архитектуру и версии с кастомными слоями, а именно кастомной свёрткой, attention-механизмом, функцией активации Swish и обобщённым пуллингом GeM. Анализ позволяет оценить влияние каждого из этих компонентов на качество обучения, обобщающую способность, вычислительную эффективность моделей и т.д.


#### Результат

| Модель         | Точность на train | Точность на test | Loss на train | Loss на test | Количество параметров | Время обучения (10 эпох, сек.) |
|----------------|-------------------|------------------|---------------|--------------|----------------------|---------------------|
| StandardCNN    | 0.8373            | 0.7270           | 0.4535        | 0.8555       | 545,098              | 421.38              |
| CustomConvCNN  | 0.8539            | 0.7202           | 0.4055        | 0.9941       | 545,100              | 490.97              |
| AttentionCNN   | 0.8469            | 0.7178           | 0.4336        | 0.9008       | 549,804              | 944.68              |
| SwishCNN       | 0.9083            | 0.7315           | 0.2581        | 0.9758       | 545,098              | 535.10              |
| GeMCNN         | 0.4280            | 0.4221           | 1.5504        | 1.5403       | 29,003               | 496.80              |
| AllCustomCNN   | 0.6193            | 0.6201           | 1.0613        | 1.0452       | 33,711               | 3021.48             |


----

StandardCNN

![LAYER_1](/lesson4/homework/plots/custom_layer_standardcnn.png)

CustomConvCNN

![LAYER_2](/lesson4/homework/plots/custom_layer_customconvcnn.png)

AttentionCNN

![LAYER_3](/lesson4/homework/plots/custom_layer_attentioncnn.png)

SwishCNN

![LAYER_4](/lesson4/homework/plots/custom_layer_swishcnn.png)

GeMCNN

![LAYER_5](/lesson4/homework/plots/custom_layer_gemcnn.png)

AllCustomCNN

![LAYER_6](/lesson4/homework/plots/custom_layer_allcustomcnn.png)

----

#### Оценка

- Точность

    Модель StandardCNN демонстрирует сбалансированное качество с точностью на обучении около 84% и на тесте около 73%, что соответствует базовому уровню для простой CNN, например, той, что использовалась в начале.

    Использование кастомного сверточного слоя в CustomConvCNN немного улучшает точность на обучении (85%) при незначительном снижении точности на тесте (~72%), что может указывать на небольшое переобучение или особенности масштабирования выхода.

    Модель с SpatialAttention (AttentionCNN) показывает близкие к стандартной модели результаты, с небольшой потерей точности на тесте (~71.8%), но с увеличением времени обучения, что связано с дополнительными вычислениями attention.

    Замена активации на Swish в SwishCNN значительно улучшает точность на обучении (90.8%) и немного повышает точность на тесте (73.1%), подтверждая преимущества плавных и дифференцируемых функций активации.

    Модель с GeMPooling (GeMCNN) демонстрирует низкую точность как на обучении (42.8%), так и на тесте (42.2%), а также высокие значения loss, что может говорить о необходимости дополнительной настройки параметра p или архитектуры при использовании GeM.

    Полная модель AllCustomCNN, объединяющая все кастомные слои, показывает средние результаты, а именно точность около 62% на обоих наборах, с высоким loss и самым долгим временем обучения, что указывает на сложность оптимизации такой комплексной архитектуры без дополнительной настройки.

- Количество параметров и время обучения

    Количество параметров у моделей с кастомными слоями примерно одинаково и близко к стандартной модели (~545 тыс.), за исключением GeMCNN и AllCustomCNN, где параметров значительно меньше (~29 тыс. и ~33 тыс. соответственно), что связано с особенностями архитектур (например, GeM сворачивает пространственный размер до 1x1).

    Время обучения варьируется - модели с attention и Swish требуют больше времени (около 500-950 секунд), GeMCNN - около 497 секунд, а AllCustomCNN - значительно дольше (около 3021 секунды), что связано с комплексностью и вычислительной нагрузкой.

- Особенности

    Кастомные слои могут улучшать качество модели, но требуют тщательной настройки и балансировки.

    Swish-активация показала заметный прирост точности, подтверждая эффективность современных функций активации.

    Attention добавляет вычислительную нагрузку, но не всегда гарантирует значительный прирост точности без дополнительной оптимизации.

    GeMPooling требует дополнительного внимания к параметрам и архитектуре, чтобы раскрыть свой потенциал.

    Сложные модели с множеством кастомных компонентов могут быть трудно оптимизируемы и требуют больше времени на обучение.



#### Вывод
- Для CIFAR-10 стандартная CNN остаётся хорошей базой с разумным балансом точности и скорости обучения.

- Внедрение кастомных сверточных слоёв и Swish-активации может улучшить качество без существенного увеличения параметров и времени.

- Attention-механизмы и GeMPooling требуют более тщательной настройки и могут увеличить время обучения без гарантированного прироста точности.

- Комплексные модели с множеством кастомных компонентов требуют аккуратного подхода к оптимизации, чтобы избежать ухудшения результатов и чрезмерных затрат времени.

- Эксперимент демонстрирует, что кастомные слои — мощный инструмент, но для их успешного применения важна правильная архитектура и настройка гиперпараметров.

